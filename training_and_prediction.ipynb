{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wkdir = \"/Users/chrisolen/Documents/uchicago_courses/deep_learning_and_image_recognition/finance/fin-portfolio-mvo/\"\n",
    "data_files = os.listdir(wkdir+'data')\n",
    "data_files.remove('.DS_Store')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(wkdir+'data/'+'data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'UST_10YR', 'UIVE_SP500VALUEETF', 'VNQ_VANGREALEST', 'USFFR',\n",
       "       'EMB_USDEMRGBOND', 'LQD_CORPBOND', 'MUB_MUNIBOND', 'SHY_1-3USTR',\n",
       "       'USDJPY', 'USDGBP', 'VIG_VANGDIV', 'IVV_SP500', 'USDRMB', 'CRUDOIL',\n",
       "       'CFE_VIX', 'EEM_MSCIEMERGING', 'USDEUR', 'XLE_ENERGYSPDR', 'SP500_GSCI',\n",
       "       'EFA_MSCIEAFE', 'TIP_TIPSBOND', 'UST_2YR', 'USDOIS', 'CHNGDP', 'USGDP',\n",
       "       'EZGDP', 'US_UNEMP', 'CHNGDP_Shock', 'USGDP_Shock', 'EZGDP_Shock',\n",
       "       'US_UNEMP_Shock', 'VNQ_VOL', 'EMB_VOL', 'LQD_VOL', 'MUB_VOL', 'VIG_VOL',\n",
       "       'IVV_VOL', 'EEM_VOL', 'EFA_VOL', 'XLE_VOL', 'SHY_VOL', 'TIP_VOL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating features into macroeconomic indictors and portfolio:\n",
    "\n",
    "econ = ['CHNGDP','USGDP','EZGDP','US_UNEMP']\n",
    "\n",
    "shock = ['CHNGDP_Shock','USGDP_Shock','EZGDP_Shock','US_UNEMP_Shock']\n",
    "\n",
    "finstruments = ['UST_10YR','USFFR','USDRMB','CRUDOIL','CFE_VIX','USDEUR','UST_2YR',\n",
    "             'SP500_GSCI','USDOIS','UIVE_SP500VALUEETF','USDJPY','USDGBP']\n",
    "\n",
    "assets = ['VNQ_VANGREALEST','EMB_USDEMRGBOND','LQD_CORPBOND',\n",
    "            'MUB_MUNIBOND','SHY_1-3USTR','VIG_VANGDIV','IVV_SP500','EEM_MSCIEMERGING',\n",
    "            'XLE_ENERGYSPDR','EFA_MSCIEAFE','TIP_TIPSBOND']\n",
    "\n",
    "asset_vols = data.columns[-11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VNQ_VANGREALEST</th>\n",
       "      <th>EMB_USDEMRGBOND</th>\n",
       "      <th>LQD_CORPBOND</th>\n",
       "      <th>MUB_MUNIBOND</th>\n",
       "      <th>SHY_1-3USTR</th>\n",
       "      <th>VIG_VANGDIV</th>\n",
       "      <th>IVV_SP500</th>\n",
       "      <th>EEM_MSCIEMERGING</th>\n",
       "      <th>XLE_ENERGYSPDR</th>\n",
       "      <th>EFA_MSCIEAFE</th>\n",
       "      <th>TIP_TIPSBOND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017507</td>\n",
       "      <td>-0.000754</td>\n",
       "      <td>-0.002072</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>-0.000484</td>\n",
       "      <td>0.008526</td>\n",
       "      <td>0.013131</td>\n",
       "      <td>0.032254</td>\n",
       "      <td>0.013098</td>\n",
       "      <td>0.012923</td>\n",
       "      <td>-0.000748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015289</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>-0.002946</td>\n",
       "      <td>-0.001470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008820</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.011717</td>\n",
       "      <td>-0.004173</td>\n",
       "      <td>-0.007582</td>\n",
       "      <td>-0.004498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000174</td>\n",
       "      <td>-0.003646</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>-0.014002</td>\n",
       "      <td>-0.012183</td>\n",
       "      <td>-0.028531</td>\n",
       "      <td>-0.012227</td>\n",
       "      <td>-0.021928</td>\n",
       "      <td>0.007393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.004192</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>-0.004140</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.006104</td>\n",
       "      <td>0.010139</td>\n",
       "      <td>0.014602</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>0.004187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.020523</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>-0.018049</td>\n",
       "      <td>-0.025074</td>\n",
       "      <td>-0.046287</td>\n",
       "      <td>-0.042255</td>\n",
       "      <td>-0.032558</td>\n",
       "      <td>0.005093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>-0.004231</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.000393</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>-0.015283</td>\n",
       "      <td>-0.018066</td>\n",
       "      <td>-0.007669</td>\n",
       "      <td>-0.024861</td>\n",
       "      <td>-0.022777</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.005662</td>\n",
       "      <td>0.008287</td>\n",
       "      <td>0.012830</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>0.004675</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>0.013426</td>\n",
       "      <td>0.013536</td>\n",
       "      <td>0.004403</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>0.003080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>-0.002143</td>\n",
       "      <td>-0.005634</td>\n",
       "      <td>-0.005080</td>\n",
       "      <td>-0.000787</td>\n",
       "      <td>-0.001059</td>\n",
       "      <td>-0.005502</td>\n",
       "      <td>-0.004503</td>\n",
       "      <td>-0.008333</td>\n",
       "      <td>-0.008928</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>-0.002738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>-0.005270</td>\n",
       "      <td>-0.000972</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>-0.014620</td>\n",
       "      <td>-0.015595</td>\n",
       "      <td>-0.007163</td>\n",
       "      <td>-0.017204</td>\n",
       "      <td>-0.008507</td>\n",
       "      <td>-0.000943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2958 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      VNQ_VANGREALEST  EMB_USDEMRGBOND  LQD_CORPBOND  MUB_MUNIBOND  \\\n",
       "1            0.017507        -0.000754     -0.002072      0.002550   \n",
       "2            0.015289         0.000881     -0.002946     -0.001470   \n",
       "3            0.000174        -0.003646      0.007275      0.001274   \n",
       "4           -0.004192         0.003451     -0.004140      0.004496   \n",
       "5           -0.020523         0.001662      0.005359      0.005446   \n",
       "...               ...              ...           ...           ...   \n",
       "2954        -0.004231        -0.000178     -0.000393      0.001754   \n",
       "2955         0.009630         0.004793      0.003922      0.002101   \n",
       "2956         0.006012         0.008728      0.004140      0.000000   \n",
       "2957        -0.002143        -0.005634     -0.005080     -0.000787   \n",
       "2958        -0.005270        -0.000972     -0.000862      0.002883   \n",
       "\n",
       "      SHY_1-3USTR  VIG_VANGDIV  IVV_SP500  EEM_MSCIEMERGING  XLE_ENERGYSPDR  \\\n",
       "1       -0.000484     0.008526   0.013131          0.032254        0.013098   \n",
       "2        0.000000     0.008820   0.006994          0.011717       -0.004173   \n",
       "3        0.002780    -0.014002  -0.012183         -0.028531       -0.012227   \n",
       "4        0.000362     0.006104   0.010139          0.014602        0.019647   \n",
       "5        0.000844    -0.018049  -0.025074         -0.046287       -0.042255   \n",
       "...           ...          ...        ...               ...             ...   \n",
       "2954     0.001061    -0.015283  -0.018066         -0.007669       -0.024861   \n",
       "2955     0.002119     0.005662   0.008287          0.012830        0.010756   \n",
       "2956    -0.000118     0.013426   0.013536          0.004403        0.006294   \n",
       "2957    -0.001059    -0.005502  -0.004503         -0.008333       -0.008928   \n",
       "2958     0.000588    -0.014620  -0.015595         -0.007163       -0.017204   \n",
       "\n",
       "      EFA_MSCIEAFE  TIP_TIPSBOND  \n",
       "1         0.012923     -0.000748  \n",
       "2        -0.007582     -0.004498  \n",
       "3        -0.021928      0.007393  \n",
       "4         0.007749      0.004187  \n",
       "5        -0.032558      0.005093  \n",
       "...            ...           ...  \n",
       "2954     -0.022777      0.000000  \n",
       "2955      0.004675      0.003176  \n",
       "2956      0.007357      0.003080  \n",
       "2957      0.001927     -0.002738  \n",
       "2958     -0.008507     -0.000943  \n",
       "\n",
       "[2958 rows x 11 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create response variable matrix to be subsequently transformed into y\n",
    "\n",
    "portfolio = data[assets]\n",
    "log_returns = np.log(portfolio/portfolio.shift(1)).dropna()\n",
    "log_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VNQ_VANGREALEST</th>\n",
       "      <th>EMB_USDEMRGBOND</th>\n",
       "      <th>LQD_CORPBOND</th>\n",
       "      <th>MUB_MUNIBOND</th>\n",
       "      <th>SHY_1-3USTR</th>\n",
       "      <th>VIG_VANGDIV</th>\n",
       "      <th>IVV_SP500</th>\n",
       "      <th>EEM_MSCIEMERGING</th>\n",
       "      <th>XLE_ENERGYSPDR</th>\n",
       "      <th>EFA_MSCIEAFE</th>\n",
       "      <th>...</th>\n",
       "      <th>EMB_VOL</th>\n",
       "      <th>LQD_VOL</th>\n",
       "      <th>MUB_VOL</th>\n",
       "      <th>VIG_VOL</th>\n",
       "      <th>IVV_VOL</th>\n",
       "      <th>EEM_VOL</th>\n",
       "      <th>EFA_VOL</th>\n",
       "      <th>XLE_VOL</th>\n",
       "      <th>SHY_VOL</th>\n",
       "      <th>TIP_VOL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.017507</td>\n",
       "      <td>-0.000754</td>\n",
       "      <td>-0.002072</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>-0.000484</td>\n",
       "      <td>0.008526</td>\n",
       "      <td>0.013131</td>\n",
       "      <td>0.032254</td>\n",
       "      <td>0.013098</td>\n",
       "      <td>0.012923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234243</td>\n",
       "      <td>0.345803</td>\n",
       "      <td>0.124218</td>\n",
       "      <td>0.506379</td>\n",
       "      <td>1.997616</td>\n",
       "      <td>0.789580</td>\n",
       "      <td>0.301695</td>\n",
       "      <td>1.721589</td>\n",
       "      <td>0.073689</td>\n",
       "      <td>0.034641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015289</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>-0.002946</td>\n",
       "      <td>-0.001470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008820</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.011717</td>\n",
       "      <td>-0.004173</td>\n",
       "      <td>-0.007582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206426</td>\n",
       "      <td>0.342953</td>\n",
       "      <td>0.109681</td>\n",
       "      <td>0.364033</td>\n",
       "      <td>1.117511</td>\n",
       "      <td>0.986725</td>\n",
       "      <td>0.128841</td>\n",
       "      <td>0.637754</td>\n",
       "      <td>0.044497</td>\n",
       "      <td>0.228210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000174</td>\n",
       "      <td>-0.003646</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>-0.014002</td>\n",
       "      <td>-0.012183</td>\n",
       "      <td>-0.028531</td>\n",
       "      <td>-0.012227</td>\n",
       "      <td>-0.021928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162948</td>\n",
       "      <td>0.319982</td>\n",
       "      <td>0.105214</td>\n",
       "      <td>0.394360</td>\n",
       "      <td>1.159634</td>\n",
       "      <td>0.886779</td>\n",
       "      <td>0.290207</td>\n",
       "      <td>0.671141</td>\n",
       "      <td>0.115845</td>\n",
       "      <td>0.287785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.004192</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>-0.004140</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.006104</td>\n",
       "      <td>0.010139</td>\n",
       "      <td>0.014602</td>\n",
       "      <td>0.019647</td>\n",
       "      <td>0.007749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156563</td>\n",
       "      <td>0.286197</td>\n",
       "      <td>0.266496</td>\n",
       "      <td>0.358497</td>\n",
       "      <td>1.143866</td>\n",
       "      <td>0.835776</td>\n",
       "      <td>0.302704</td>\n",
       "      <td>0.642441</td>\n",
       "      <td>0.128374</td>\n",
       "      <td>0.452968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.020523</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>-0.018049</td>\n",
       "      <td>-0.025074</td>\n",
       "      <td>-0.046287</td>\n",
       "      <td>-0.042255</td>\n",
       "      <td>-0.032558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191924</td>\n",
       "      <td>0.368284</td>\n",
       "      <td>0.472737</td>\n",
       "      <td>0.516701</td>\n",
       "      <td>1.517778</td>\n",
       "      <td>1.132523</td>\n",
       "      <td>0.545225</td>\n",
       "      <td>1.298218</td>\n",
       "      <td>0.154045</td>\n",
       "      <td>0.688876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>-0.004231</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>-0.000393</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>-0.015283</td>\n",
       "      <td>-0.018066</td>\n",
       "      <td>-0.007669</td>\n",
       "      <td>-0.024861</td>\n",
       "      <td>-0.022777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.507070</td>\n",
       "      <td>0.183439</td>\n",
       "      <td>0.079183</td>\n",
       "      <td>1.388910</td>\n",
       "      <td>3.723606</td>\n",
       "      <td>0.336348</td>\n",
       "      <td>0.355992</td>\n",
       "      <td>1.403093</td>\n",
       "      <td>0.052249</td>\n",
       "      <td>0.066858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955</th>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.005662</td>\n",
       "      <td>0.008287</td>\n",
       "      <td>0.012830</td>\n",
       "      <td>0.010756</td>\n",
       "      <td>0.004675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448687</td>\n",
       "      <td>0.258882</td>\n",
       "      <td>0.157575</td>\n",
       "      <td>1.382292</td>\n",
       "      <td>3.616485</td>\n",
       "      <td>0.232056</td>\n",
       "      <td>0.379803</td>\n",
       "      <td>1.379554</td>\n",
       "      <td>0.112472</td>\n",
       "      <td>0.190971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>0.006012</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000118</td>\n",
       "      <td>0.013426</td>\n",
       "      <td>0.013536</td>\n",
       "      <td>0.004403</td>\n",
       "      <td>0.006294</td>\n",
       "      <td>0.007357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650769</td>\n",
       "      <td>0.420868</td>\n",
       "      <td>0.189473</td>\n",
       "      <td>1.318283</td>\n",
       "      <td>3.465087</td>\n",
       "      <td>0.277993</td>\n",
       "      <td>0.350471</td>\n",
       "      <td>1.049843</td>\n",
       "      <td>0.125579</td>\n",
       "      <td>0.332310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2957</th>\n",
       "      <td>-0.002143</td>\n",
       "      <td>-0.005634</td>\n",
       "      <td>-0.005080</td>\n",
       "      <td>-0.000787</td>\n",
       "      <td>-0.001059</td>\n",
       "      <td>-0.005502</td>\n",
       "      <td>-0.004503</td>\n",
       "      <td>-0.008333</td>\n",
       "      <td>-0.008928</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642051</td>\n",
       "      <td>0.415175</td>\n",
       "      <td>0.187563</td>\n",
       "      <td>0.907981</td>\n",
       "      <td>2.591067</td>\n",
       "      <td>0.260615</td>\n",
       "      <td>0.227662</td>\n",
       "      <td>0.535416</td>\n",
       "      <td>0.114761</td>\n",
       "      <td>0.308982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2958</th>\n",
       "      <td>-0.005270</td>\n",
       "      <td>-0.000972</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>-0.014620</td>\n",
       "      <td>-0.015595</td>\n",
       "      <td>-0.007163</td>\n",
       "      <td>-0.017204</td>\n",
       "      <td>-0.008507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555761</td>\n",
       "      <td>0.380039</td>\n",
       "      <td>0.174413</td>\n",
       "      <td>1.021802</td>\n",
       "      <td>2.795196</td>\n",
       "      <td>0.295973</td>\n",
       "      <td>0.147241</td>\n",
       "      <td>0.573699</td>\n",
       "      <td>0.073959</td>\n",
       "      <td>0.261094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2958 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      VNQ_VANGREALEST  EMB_USDEMRGBOND  LQD_CORPBOND  MUB_MUNIBOND  \\\n",
       "1            0.017507        -0.000754     -0.002072      0.002550   \n",
       "2            0.015289         0.000881     -0.002946     -0.001470   \n",
       "3            0.000174        -0.003646      0.007275      0.001274   \n",
       "4           -0.004192         0.003451     -0.004140      0.004496   \n",
       "5           -0.020523         0.001662      0.005359      0.005446   \n",
       "...               ...              ...           ...           ...   \n",
       "2954        -0.004231        -0.000178     -0.000393      0.001754   \n",
       "2955         0.009630         0.004793      0.003922      0.002101   \n",
       "2956         0.006012         0.008728      0.004140      0.000000   \n",
       "2957        -0.002143        -0.005634     -0.005080     -0.000787   \n",
       "2958        -0.005270        -0.000972     -0.000862      0.002883   \n",
       "\n",
       "      SHY_1-3USTR  VIG_VANGDIV  IVV_SP500  EEM_MSCIEMERGING  XLE_ENERGYSPDR  \\\n",
       "1       -0.000484     0.008526   0.013131          0.032254        0.013098   \n",
       "2        0.000000     0.008820   0.006994          0.011717       -0.004173   \n",
       "3        0.002780    -0.014002  -0.012183         -0.028531       -0.012227   \n",
       "4        0.000362     0.006104   0.010139          0.014602        0.019647   \n",
       "5        0.000844    -0.018049  -0.025074         -0.046287       -0.042255   \n",
       "...           ...          ...        ...               ...             ...   \n",
       "2954     0.001061    -0.015283  -0.018066         -0.007669       -0.024861   \n",
       "2955     0.002119     0.005662   0.008287          0.012830        0.010756   \n",
       "2956    -0.000118     0.013426   0.013536          0.004403        0.006294   \n",
       "2957    -0.001059    -0.005502  -0.004503         -0.008333       -0.008928   \n",
       "2958     0.000588    -0.014620  -0.015595         -0.007163       -0.017204   \n",
       "\n",
       "      EFA_MSCIEAFE  ...   EMB_VOL   LQD_VOL   MUB_VOL   VIG_VOL   IVV_VOL  \\\n",
       "1         0.012923  ...  0.234243  0.345803  0.124218  0.506379  1.997616   \n",
       "2        -0.007582  ...  0.206426  0.342953  0.109681  0.364033  1.117511   \n",
       "3        -0.021928  ...  0.162948  0.319982  0.105214  0.394360  1.159634   \n",
       "4         0.007749  ...  0.156563  0.286197  0.266496  0.358497  1.143866   \n",
       "5        -0.032558  ...  0.191924  0.368284  0.472737  0.516701  1.517778   \n",
       "...            ...  ...       ...       ...       ...       ...       ...   \n",
       "2954     -0.022777  ...  0.507070  0.183439  0.079183  1.388910  3.723606   \n",
       "2955      0.004675  ...  0.448687  0.258882  0.157575  1.382292  3.616485   \n",
       "2956      0.007357  ...  0.650769  0.420868  0.189473  1.318283  3.465087   \n",
       "2957      0.001927  ...  0.642051  0.415175  0.187563  0.907981  2.591067   \n",
       "2958     -0.008507  ...  0.555761  0.380039  0.174413  1.021802  2.795196   \n",
       "\n",
       "       EEM_VOL   EFA_VOL   XLE_VOL   SHY_VOL   TIP_VOL  \n",
       "1     0.789580  0.301695  1.721589  0.073689  0.034641  \n",
       "2     0.986725  0.128841  0.637754  0.044497  0.228210  \n",
       "3     0.886779  0.290207  0.671141  0.115845  0.287785  \n",
       "4     0.835776  0.302704  0.642441  0.128374  0.452968  \n",
       "5     1.132523  0.545225  1.298218  0.154045  0.688876  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "2954  0.336348  0.355992  1.403093  0.052249  0.066858  \n",
       "2955  0.232056  0.379803  1.379554  0.112472  0.190971  \n",
       "2956  0.277993  0.350471  1.049843  0.125579  0.332310  \n",
       "2957  0.260615  0.227662  0.535416  0.114761  0.308982  \n",
       "2958  0.295973  0.147241  0.573699  0.073959  0.261094  \n",
       "\n",
       "[2958 rows x 42 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create features matrix to be subsequently transformed into X\n",
    "\n",
    "# Log returns of assets\n",
    "asset_features = np.log(data[assets]/data[assets].shift(1))\n",
    "\n",
    "# Scaled macroeconomic factors\n",
    "scale = StandardScaler()\n",
    "econ_features = pd.DataFrame(scale.fit_transform(data[econ]), columns = data[econ].columns)\n",
    "\n",
    "# Log returns of financial instruments\n",
    "finstruments_features = np.log(data[finstruments]/data[finstruments].shift(1))\n",
    "\n",
    "features = pd.concat([asset_features, econ_features, finstruments_features, \n",
    "                     data[shock], data[asset_vols]], axis = 1).dropna()\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition:\n",
    "\n",
    "batch_size = 5\n",
    "n_batches = math.floor(features.shape[0] / batch_size) # number of batches in an epoch\n",
    "n_timesteps = 10 # length of series used for prediction (i.e. how many days we're predict off of)\n",
    "\n",
    "n_features = features.shape[1] # number of features used for prediction\n",
    "n_predicted_vars = log_returns.shape[1] # number of asset values being predicted\n",
    "\n",
    "n_epochs = 5\n",
    "look_ahead_time = 1 # number of days in advance we will predict\n",
    "validation_split = 0.1\n",
    "checkpoint_dir = \"model_checkpoints\"\n",
    "\n",
    "burn_in_length = 200\n",
    "burn_in_epochs = 50\n",
    "\n",
    "delta = np.array([1.5])\n",
    "min_weight = np.array([0.01])\n",
    "vol_window = 10\n",
    "\n",
    "daily_portfolio_weights = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding window:\n",
    "            \n",
    "def sliding_window(features, log_returns, end_index, n_timesteps, look_ahead_time):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes in features matrix and response var (i.e. log_returns) matrix\n",
    "    RETURNS: X and y matrices for training\n",
    "    \n",
    "    end_index: the most recent time instance used for prediction\n",
    "    n_timesteps: length of series used for prediction\n",
    "    look_ahead_time: how many days ahead we are predicting\n",
    "    \"\"\"\n",
    "    \n",
    "    # start index is derived from end_index - n_timesteps\n",
    "    # X slices up to but does not include end_index\n",
    "    X = np.array(features.iloc[(end_index-n_timesteps):end_index, :])                              \n",
    "    \n",
    "    # y includes end_index and slices up to but does not include end_index + look_ahead_time\n",
    "    # thus, if look_ahead_time = 1, y will only include one day for prediction\n",
    "    y = np.array(log_returns.iloc[end_index+look_ahead_time-1, :]) # currently can only make predictions of a single time step\n",
    "    \n",
    "    # return (X:[n_timesteps,n_features], y:[look_ahead_time, n_assets])\n",
    "    return X, y\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_epoch(features, log_returns, n_timesteps, look_ahead_time):\n",
    "    \n",
    "    # begin with empty arrays to which we will append \n",
    "    X = np.array([]) \n",
    "    y = np.array([])\n",
    "    \n",
    "    window_count = 0\n",
    "    \n",
    "    for i in range(len(features)-n_timesteps):\n",
    "        \n",
    "        end_index = i + n_timesteps\n",
    "        \n",
    "        # pull out one window\n",
    "        X_one, y_one = sliding_window(features, log_returns, end_index, n_timesteps, look_ahead_time)\n",
    "        \n",
    "        ### append sliding windows ###\n",
    "        # append X_one:[n_timesteps,n_features] to batch ndarray X\n",
    "        X = np.append(X, X_one)\n",
    "        # append y_one:[look_ahead_time, n_assets] to batch ndarray y\n",
    "        y = np.append(y, y_one)\n",
    "        \n",
    "        # count the number of windows (i.e. training instances)\n",
    "        window_count += 1\n",
    "     \n",
    "    \n",
    "    # reshape training vectors given window_count\n",
    "    X = X.reshape(window_count, n_timesteps, features.shape[1])    \n",
    "    y = y.reshape(window_count, log_returns.shape[1])\n",
    "        \n",
    "    return X, y\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 10, 42)]          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 11)                2376      \n",
      "_________________________________________________________________\n",
      "dense_layer (Dense)          (None, 11)                132       \n",
      "=================================================================\n",
      "Total params: 2,508\n",
      "Trainable params: 2,508\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build model:\n",
    "\n",
    "sliding_window_input = tf.keras.layers.Input(shape=(n_timesteps, n_features,), \n",
    "                                             name = \"input_layer\")\n",
    "lstm_out = tf.keras.layers.LSTM(n_predicted_vars, \n",
    "                                activation='tanh', recurrent_activation='sigmoid',\n",
    "                                dropout=0.2, stateful=False,\n",
    "                                name = \"lstm\")(sliding_window_input)\n",
    "dense_out = tf.keras.layers.Dense(n_predicted_vars, \n",
    "                                  activation='relu', name = \"dense_layer\")(lstm_out)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=sliding_window_input, outputs=dense_out)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), \n",
    "              loss='mse')    \n",
    "model.summary()\n",
    "\n",
    "checkpoints = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir + \"/saved_model.hdf5\", verbose=1, \n",
    "                                                     save_best_only=True, \n",
    "                                                     mode='auto', \n",
    "                                                     save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 171 samples, validate on 19 samples\n",
      "Epoch 1/50\n",
      "125/171 [====================>.........] - ETA: 0s - loss: 0.0376\n",
      "Epoch 00001: val_loss improved from inf to 0.03206, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 2s 10ms/sample - loss: 0.0376 - val_loss: 0.0321\n",
      "Epoch 2/50\n",
      "155/171 [==========================>...] - ETA: 0s - loss: 0.0370\n",
      "Epoch 00002: val_loss improved from 0.03206 to 0.03124, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0370 - val_loss: 0.0312\n",
      "Epoch 3/50\n",
      "170/171 [============================>.] - ETA: 0s - loss: 0.0337\n",
      "Epoch 00003: val_loss improved from 0.03124 to 0.03043, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0336 - val_loss: 0.0304\n",
      "Epoch 4/50\n",
      "150/171 [=========================>....] - ETA: 0s - loss: 0.0342\n",
      "Epoch 00004: val_loss improved from 0.03043 to 0.02965, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 896us/sample - loss: 0.0338 - val_loss: 0.0296\n",
      "Epoch 5/50\n",
      "150/171 [=========================>....] - ETA: 0s - loss: 0.0334\n",
      "Epoch 00005: val_loss improved from 0.02965 to 0.02887, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 876us/sample - loss: 0.0336 - val_loss: 0.0289\n",
      "Epoch 6/50\n",
      "155/171 [==========================>...] - ETA: 0s - loss: 0.0333\n",
      "Epoch 00006: val_loss improved from 0.02887 to 0.02812, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 864us/sample - loss: 0.0333 - val_loss: 0.0281\n",
      "Epoch 7/50\n",
      "150/171 [=========================>....] - ETA: 0s - loss: 0.0311\n",
      "Epoch 00007: val_loss improved from 0.02812 to 0.02747, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 870us/sample - loss: 0.0310 - val_loss: 0.0275\n",
      "Epoch 8/50\n",
      "150/171 [=========================>....] - ETA: 0s - loss: 0.0319\n",
      "Epoch 00008: val_loss improved from 0.02747 to 0.02677, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 886us/sample - loss: 0.0317 - val_loss: 0.0268\n",
      "Epoch 9/50\n",
      "155/171 [==========================>...] - ETA: 0s - loss: 0.0297\n",
      "Epoch 00009: val_loss improved from 0.02677 to 0.02613, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 865us/sample - loss: 0.0303 - val_loss: 0.0261\n",
      "Epoch 10/50\n",
      "150/171 [=========================>....] - ETA: 0s - loss: 0.0291\n",
      "Epoch 00010: val_loss improved from 0.02613 to 0.02552, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 865us/sample - loss: 0.0292 - val_loss: 0.0255\n",
      "Epoch 11/50\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 0.0300\n",
      "Epoch 00011: val_loss improved from 0.02552 to 0.02488, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 917us/sample - loss: 0.0298 - val_loss: 0.0249\n",
      "Epoch 12/50\n",
      "140/171 [=======================>......] - ETA: 0s - loss: 0.0283\n",
      "Epoch 00012: val_loss improved from 0.02488 to 0.02427, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 992us/sample - loss: 0.0282 - val_loss: 0.0243\n",
      "Epoch 13/50\n",
      "170/171 [============================>.] - ETA: 0s - loss: 0.0272\n",
      "Epoch 00013: val_loss improved from 0.02427 to 0.02372, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0272 - val_loss: 0.0237\n",
      "Epoch 14/50\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 0.0277\n",
      "Epoch 00014: val_loss improved from 0.02372 to 0.02315, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0273 - val_loss: 0.0231\n",
      "Epoch 15/50\n",
      "125/171 [====================>.........] - ETA: 0s - loss: 0.0260\n",
      "Epoch 00015: val_loss improved from 0.02315 to 0.02262, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0258 - val_loss: 0.0226\n",
      "Epoch 16/50\n",
      "125/171 [====================>.........] - ETA: 0s - loss: 0.0238\n",
      "Epoch 00016: val_loss improved from 0.02262 to 0.02213, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0241 - val_loss: 0.0221\n",
      "Epoch 17/50\n",
      "135/171 [======================>.......] - ETA: 0s - loss: 0.0248\n",
      "Epoch 00017: val_loss improved from 0.02213 to 0.02164, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 965us/sample - loss: 0.0245 - val_loss: 0.0216\n",
      "Epoch 18/50\n",
      "140/171 [=======================>......] - ETA: 0s - loss: 0.0230\n",
      "Epoch 00018: val_loss improved from 0.02164 to 0.02117, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 975us/sample - loss: 0.0232 - val_loss: 0.0212\n",
      "Epoch 19/50\n",
      "140/171 [=======================>......] - ETA: 0s - loss: 0.0231\n",
      "Epoch 00019: val_loss improved from 0.02117 to 0.02073, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 944us/sample - loss: 0.0228 - val_loss: 0.0207\n",
      "Epoch 20/50\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 0.0224\n",
      "Epoch 00020: val_loss improved from 0.02073 to 0.02030, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 922us/sample - loss: 0.0224 - val_loss: 0.0203\n",
      "Epoch 21/50\n",
      "140/171 [=======================>......] - ETA: 0s - loss: 0.0228\n",
      "Epoch 00021: val_loss improved from 0.02030 to 0.01985, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0226 - val_loss: 0.0199\n",
      "Epoch 22/50\n",
      "120/171 [====================>.........] - ETA: 0s - loss: 0.0212\n",
      "Epoch 00022: val_loss improved from 0.01985 to 0.01946, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0210 - val_loss: 0.0195\n",
      "Epoch 23/50\n",
      "150/171 [=========================>....] - ETA: 0s - loss: 0.0207\n",
      "Epoch 00023: val_loss improved from 0.01946 to 0.01907, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 866us/sample - loss: 0.0208 - val_loss: 0.0191\n",
      "Epoch 24/50\n",
      "150/171 [=========================>....] - ETA: 0s - loss: 0.0206\n",
      "Epoch 00024: val_loss improved from 0.01907 to 0.01866, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 868us/sample - loss: 0.0205 - val_loss: 0.0187\n",
      "Epoch 25/50\n",
      "125/171 [====================>.........] - ETA: 0s - loss: 0.0193\n",
      "Epoch 00025: val_loss improved from 0.01866 to 0.01826, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 954us/sample - loss: 0.0193 - val_loss: 0.0183\n",
      "Epoch 26/50\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 0.0192\n",
      "Epoch 00026: val_loss improved from 0.01826 to 0.01787, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 965us/sample - loss: 0.0191 - val_loss: 0.0179\n",
      "Epoch 27/50\n",
      "135/171 [======================>.......] - ETA: 0s - loss: 0.0185\n",
      "Epoch 00027: val_loss improved from 0.01787 to 0.01753, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 981us/sample - loss: 0.0191 - val_loss: 0.0175\n",
      "Epoch 28/50\n",
      "140/171 [=======================>......] - ETA: 0s - loss: 0.0172\n",
      "Epoch 00028: val_loss improved from 0.01753 to 0.01722, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 969us/sample - loss: 0.0174 - val_loss: 0.0172\n",
      "Epoch 29/50\n",
      "130/171 [=====================>........] - ETA: 0s - loss: 0.0177\n",
      "Epoch 00029: val_loss improved from 0.01722 to 0.01691, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 999us/sample - loss: 0.0174 - val_loss: 0.0169\n",
      "Epoch 30/50\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 0.0169\n",
      "Epoch 00030: val_loss improved from 0.01691 to 0.01659, saving model to model_checkpoints/saved_model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171/171 [==============================] - 0s 891us/sample - loss: 0.0168 - val_loss: 0.0166\n",
      "Epoch 31/50\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 0.0166\n",
      "Epoch 00031: val_loss improved from 0.01659 to 0.01626, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 892us/sample - loss: 0.0164 - val_loss: 0.0163\n",
      "Epoch 32/50\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 0.0160\n",
      "Epoch 00032: val_loss improved from 0.01626 to 0.01597, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 932us/sample - loss: 0.0161 - val_loss: 0.0160\n",
      "Epoch 33/50\n",
      "130/171 [=====================>........] - ETA: 0s - loss: 0.0154\n",
      "Epoch 00033: val_loss improved from 0.01597 to 0.01569, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0151 - val_loss: 0.0157\n",
      "Epoch 34/50\n",
      "155/171 [==========================>...] - ETA: 0s - loss: 0.0153\n",
      "Epoch 00034: val_loss improved from 0.01569 to 0.01542, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 852us/sample - loss: 0.0155 - val_loss: 0.0154\n",
      "Epoch 35/50\n",
      "155/171 [==========================>...] - ETA: 0s - loss: 0.0134\n",
      "Epoch 00035: val_loss improved from 0.01542 to 0.01515, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 832us/sample - loss: 0.0138 - val_loss: 0.0152\n",
      "Epoch 36/50\n",
      "160/171 [===========================>..] - ETA: 0s - loss: 0.0139\n",
      "Epoch 00036: val_loss improved from 0.01515 to 0.01488, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 940us/sample - loss: 0.0140 - val_loss: 0.0149\n",
      "Epoch 37/50\n",
      "160/171 [===========================>..] - ETA: 0s - loss: 0.0141\n",
      "Epoch 00037: val_loss improved from 0.01488 to 0.01462, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0142 - val_loss: 0.0146\n",
      "Epoch 38/50\n",
      "120/171 [====================>.........] - ETA: 0s - loss: 0.0135\n",
      "Epoch 00038: val_loss improved from 0.01462 to 0.01439, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0134 - val_loss: 0.0144\n",
      "Epoch 39/50\n",
      "170/171 [============================>.] - ETA: 0s - loss: 0.0130\n",
      "Epoch 00039: val_loss improved from 0.01439 to 0.01417, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0129 - val_loss: 0.0142\n",
      "Epoch 40/50\n",
      "135/171 [======================>.......] - ETA: 0s - loss: 0.0130\n",
      "Epoch 00040: val_loss improved from 0.01417 to 0.01395, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 1ms/sample - loss: 0.0126 - val_loss: 0.0140\n",
      "Epoch 41/50\n",
      "130/171 [=====================>........] - ETA: 0s - loss: 0.0126\n",
      "Epoch 00041: val_loss improved from 0.01395 to 0.01375, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 929us/sample - loss: 0.0118 - val_loss: 0.0137\n",
      "Epoch 42/50\n",
      "135/171 [======================>.......] - ETA: 0s - loss: 0.0117\n",
      "Epoch 00042: val_loss improved from 0.01375 to 0.01352, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 940us/sample - loss: 0.0120 - val_loss: 0.0135\n",
      "Epoch 43/50\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 0.0114\n",
      "Epoch 00043: val_loss improved from 0.01352 to 0.01330, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 914us/sample - loss: 0.0114 - val_loss: 0.0133\n",
      "Epoch 44/50\n",
      "150/171 [=========================>....] - ETA: 0s - loss: 0.0108\n",
      "Epoch 00044: val_loss improved from 0.01330 to 0.01310, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 879us/sample - loss: 0.0110 - val_loss: 0.0131\n",
      "Epoch 45/50\n",
      "155/171 [==========================>...] - ETA: 0s - loss: 0.0106\n",
      "Epoch 00045: val_loss improved from 0.01310 to 0.01288, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 856us/sample - loss: 0.0106 - val_loss: 0.0129\n",
      "Epoch 46/50\n",
      "150/171 [=========================>....] - ETA: 0s - loss: 0.0100\n",
      "Epoch 00046: val_loss improved from 0.01288 to 0.01271, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 846us/sample - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 47/50\n",
      "150/171 [=========================>....] - ETA: 0s - loss: 0.0101\n",
      "Epoch 00047: val_loss improved from 0.01271 to 0.01250, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 856us/sample - loss: 0.0102 - val_loss: 0.0125\n",
      "Epoch 48/50\n",
      "150/171 [=========================>....] - ETA: 0s - loss: 0.0094\n",
      "Epoch 00048: val_loss improved from 0.01250 to 0.01234, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 874us/sample - loss: 0.0098 - val_loss: 0.0123\n",
      "Epoch 49/50\n",
      "150/171 [=========================>....] - ETA: 0s - loss: 0.0096\n",
      "Epoch 00049: val_loss improved from 0.01234 to 0.01215, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 855us/sample - loss: 0.0095 - val_loss: 0.0121\n",
      "Epoch 50/50\n",
      "155/171 [==========================>...] - ETA: 0s - loss: 0.0094\n",
      "Epoch 00050: val_loss improved from 0.01215 to 0.01197, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 819us/sample - loss: 0.0092 - val_loss: 0.0120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a31331ad0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# burn in period\n",
    "\n",
    "training_features = features.iloc[0:burn_in_length,:]\n",
    "training_response = log_returns.iloc[0:burn_in_length,:]\n",
    "\n",
    "X, y = generate_epoch(training_features, training_response, n_timesteps, look_ahead_time)\n",
    "\n",
    "model.fit(X, y,\n",
    "          validation_split = validation_split,\n",
    "          epochs = burn_in_epochs,\n",
    "          batch_size = batch_size,\n",
    "          callbacks = [checkpoints])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 171 samples, validate on 19 samples\n",
      "Epoch 1/5\n",
      "140/171 [=======================>......] - ETA: 0s - loss: 0.0119\n",
      "Epoch 00001: val_loss improved from 0.01197 to 0.01172, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 1s 6ms/sample - loss: 0.0123 - val_loss: 0.0117\n",
      "Epoch 2/5\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 0.0120\n",
      "Epoch 00002: val_loss improved from 0.01172 to 0.01141, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 930us/sample - loss: 0.0125 - val_loss: 0.0114\n",
      "Epoch 3/5\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00003: val_loss improved from 0.01141 to 0.01115, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 912us/sample - loss: 0.0125 - val_loss: 0.0111\n",
      "Epoch 4/5\n",
      "145/171 [========================>.....] - ETA: 0s - loss: 0.0106\n",
      "Epoch 00004: val_loss improved from 0.01115 to 0.01093, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 908us/sample - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 5/5\n",
      "165/171 [===========================>..] - ETA: 0s - loss: 0.0114\n",
      "Epoch 00005: val_loss improved from 0.01093 to 0.01070, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 801us/sample - loss: 0.0115 - val_loss: 0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrisolen/anaconda3/lib/python3.7/site-packages/julia/core.py:689: FutureWarning: Accessing `Julia().<name>` to obtain Julia objects is deprecated.  Use `from julia import Main; Main.<name>` or `jl = Julia(); jl.eval('<name>')`.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 847 ms, total: 1min 18s\n",
      "Wall time: 1min 18s\n",
      "Train on 171 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "135/171 [======================>.......] - ETA: 0s - loss: 0.0109\n",
      "Epoch 00001: val_loss improved from 0.01070 to 0.01062, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 1s 5ms/sample - loss: 0.0106 - val_loss: 0.0106\n",
      "Epoch 2/5\n",
      "155/171 [==========================>...] - ETA: 0s - loss: 0.0110\n",
      "Epoch 00002: val_loss improved from 0.01062 to 0.01040, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 869us/sample - loss: 0.0109 - val_loss: 0.0104\n",
      "Epoch 3/5\n",
      "155/171 [==========================>...] - ETA: 0s - loss: 0.0108\n",
      "Epoch 00003: val_loss improved from 0.01040 to 0.01020, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 864us/sample - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 4/5\n",
      "160/171 [===========================>..] - ETA: 0s - loss: 0.0105\n",
      "Epoch 00004: val_loss improved from 0.01020 to 0.01001, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 835us/sample - loss: 0.0103 - val_loss: 0.0100\n",
      "Epoch 5/5\n",
      "155/171 [==========================>...] - ETA: 0s - loss: 0.0091\n",
      "Epoch 00005: val_loss improved from 0.01001 to 0.00979, saving model to model_checkpoints/saved_model.hdf5\n",
      "171/171 [==============================] - 0s 841us/sample - loss: 0.0093 - val_loss: 0.0098\n",
      "CPU times: user 128 ms, sys: 3.11 ms, total: 131 ms\n",
      "Wall time: 132 ms\n",
      "Train on 172 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "155/172 [==========================>...] - ETA: 0s - loss: 0.0101\n",
      "Epoch 00001: val_loss did not improve from 0.00979\n",
      "172/172 [==============================] - 1s 5ms/sample - loss: 0.0098 - val_loss: 0.0103\n",
      "Epoch 2/5\n",
      "145/172 [========================>.....] - ETA: 0s - loss: 0.0097\n",
      "Epoch 00002: val_loss did not improve from 0.00979\n",
      "172/172 [==============================] - 0s 830us/sample - loss: 0.0099 - val_loss: 0.0101\n",
      "Epoch 3/5\n",
      "135/172 [======================>.......] - ETA: 0s - loss: 0.0094\n",
      "Epoch 00003: val_loss did not improve from 0.00979\n",
      "172/172 [==============================] - 0s 885us/sample - loss: 0.0093 - val_loss: 0.0099\n",
      "Epoch 4/5\n",
      "130/172 [=====================>........] - ETA: 0s - loss: 0.0085\n",
      "Epoch 00004: val_loss improved from 0.00979 to 0.00972, saving model to model_checkpoints/saved_model.hdf5\n",
      "172/172 [==============================] - 0s 965us/sample - loss: 0.0085 - val_loss: 0.0097\n",
      "Epoch 5/5\n",
      "155/172 [==========================>...] - ETA: 0s - loss: 0.0083\n",
      "Epoch 00005: val_loss improved from 0.00972 to 0.00954, saving model to model_checkpoints/saved_model.hdf5\n",
      "172/172 [==============================] - 0s 862us/sample - loss: 0.0085 - val_loss: 0.0095\n",
      "CPU times: user 96.6 ms, sys: 2.78 ms, total: 99.4 ms\n",
      "Wall time: 100 ms\n",
      "Train on 173 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "140/173 [=======================>......] - ETA: 0s - loss: 0.0088\n",
      "Epoch 00001: val_loss improved from 0.00954 to 0.00952, saving model to model_checkpoints/saved_model.hdf5\n",
      "173/173 [==============================] - 1s 5ms/sample - loss: 0.0089 - val_loss: 0.0095\n",
      "Epoch 2/5\n",
      "155/173 [=========================>....] - ETA: 0s - loss: 0.0080\n",
      "Epoch 00002: val_loss improved from 0.00952 to 0.00938, saving model to model_checkpoints/saved_model.hdf5\n",
      "173/173 [==============================] - 0s 847us/sample - loss: 0.0080 - val_loss: 0.0094\n",
      "Epoch 3/5\n",
      "155/173 [=========================>....] - ETA: 0s - loss: 0.0089\n",
      "Epoch 00003: val_loss improved from 0.00938 to 0.00923, saving model to model_checkpoints/saved_model.hdf5\n",
      "173/173 [==============================] - 0s 844us/sample - loss: 0.0087 - val_loss: 0.0092\n",
      "Epoch 4/5\n",
      "155/173 [=========================>....] - ETA: 0s - loss: 0.0082\n",
      "Epoch 00004: val_loss improved from 0.00923 to 0.00909, saving model to model_checkpoints/saved_model.hdf5\n",
      "173/173 [==============================] - 0s 846us/sample - loss: 0.0081 - val_loss: 0.0091\n",
      "Epoch 5/5\n",
      "145/173 [========================>.....] - ETA: 0s - loss: 0.0072\n",
      "Epoch 00005: val_loss improved from 0.00909 to 0.00897, saving model to model_checkpoints/saved_model.hdf5\n",
      "173/173 [==============================] - 0s 865us/sample - loss: 0.0074 - val_loss: 0.0090\n",
      "CPU times: user 94.5 ms, sys: 2.42 ms, total: 96.9 ms\n",
      "Wall time: 96.9 ms\n",
      "Train on 174 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "135/174 [======================>.......] - ETA: 0s - loss: 0.0072\n",
      "Epoch 00001: val_loss did not improve from 0.00897\n",
      "174/174 [==============================] - 1s 5ms/sample - loss: 0.0074 - val_loss: 0.0094\n",
      "Epoch 2/5\n",
      "165/174 [===========================>..] - ETA: 0s - loss: 0.0073\n",
      "Epoch 00002: val_loss did not improve from 0.00897\n",
      "174/174 [==============================] - 0s 751us/sample - loss: 0.0074 - val_loss: 0.0092\n",
      "Epoch 3/5\n",
      "160/174 [==========================>...] - ETA: 0s - loss: 0.0073\n",
      "Epoch 00003: val_loss did not improve from 0.00897\n",
      "174/174 [==============================] - 0s 744us/sample - loss: 0.0071 - val_loss: 0.0091\n",
      "Epoch 4/5\n",
      "165/174 [===========================>..] - ETA: 0s - loss: 0.0068\n",
      "Epoch 00004: val_loss did not improve from 0.00897\n",
      "174/174 [==============================] - 0s 725us/sample - loss: 0.0069 - val_loss: 0.0090\n",
      "Epoch 5/5\n",
      "160/174 [==========================>...] - ETA: 0s - loss: 0.0065\n",
      "Epoch 00005: val_loss improved from 0.00897 to 0.00889, saving model to model_checkpoints/saved_model.hdf5\n",
      "174/174 [==============================] - 0s 814us/sample - loss: 0.0065 - val_loss: 0.0089\n",
      "CPU times: user 117 ms, sys: 2.71 ms, total: 120 ms\n",
      "Wall time: 120 ms\n",
      "Train on 175 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "140/175 [=======================>......] - ETA: 0s - loss: 0.0069\n",
      "Epoch 00001: val_loss improved from 0.00889 to 0.00883, saving model to model_checkpoints/saved_model.hdf5\n",
      "175/175 [==============================] - 1s 3ms/sample - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 2/5\n",
      "150/175 [========================>.....] - ETA: 0s - loss: 0.0071\n",
      "Epoch 00002: val_loss improved from 0.00883 to 0.00868, saving model to model_checkpoints/saved_model.hdf5\n",
      "175/175 [==============================] - 0s 883us/sample - loss: 0.0071 - val_loss: 0.0087\n",
      "Epoch 3/5\n",
      "145/175 [=======================>......] - ETA: 0s - loss: 0.0066\n",
      "Epoch 00003: val_loss improved from 0.00868 to 0.00855, saving model to model_checkpoints/saved_model.hdf5\n",
      "175/175 [==============================] - 0s 887us/sample - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 4/5\n",
      "165/175 [===========================>..] - ETA: 0s - loss: 0.0060\n",
      "Epoch 00004: val_loss improved from 0.00855 to 0.00843, saving model to model_checkpoints/saved_model.hdf5\n",
      "175/175 [==============================] - 0s 806us/sample - loss: 0.0059 - val_loss: 0.0084\n",
      "Epoch 5/5\n",
      "150/175 [========================>.....] - ETA: 0s - loss: 0.0060\n",
      "Epoch 00005: val_loss improved from 0.00843 to 0.00830, saving model to model_checkpoints/saved_model.hdf5\n",
      "175/175 [==============================] - 0s 863us/sample - loss: 0.0062 - val_loss: 0.0083\n",
      "CPU times: user 88.7 ms, sys: 2.05 ms, total: 90.7 ms\n",
      "Wall time: 90.6 ms\n",
      "Train on 176 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "110/176 [=================>............] - ETA: 0s - loss: 0.0064\n",
      "Epoch 00001: val_loss improved from 0.00830 to 0.00790, saving model to model_checkpoints/saved_model.hdf5\n",
      "176/176 [==============================] - 1s 5ms/sample - loss: 0.0061 - val_loss: 0.0079\n",
      "Epoch 2/5\n",
      "130/176 [=====================>........] - ETA: 0s - loss: 0.0058\n",
      "Epoch 00002: val_loss improved from 0.00790 to 0.00780, saving model to model_checkpoints/saved_model.hdf5\n",
      "176/176 [==============================] - 0s 983us/sample - loss: 0.0055 - val_loss: 0.0078\n",
      "Epoch 3/5\n",
      "150/176 [========================>.....] - ETA: 0s - loss: 0.0054\n",
      "Epoch 00003: val_loss improved from 0.00780 to 0.00771, saving model to model_checkpoints/saved_model.hdf5\n",
      "176/176 [==============================] - 0s 902us/sample - loss: 0.0056 - val_loss: 0.0077\n",
      "Epoch 4/5\n",
      "140/176 [======================>.......] - ETA: 0s - loss: 0.0055\n",
      "Epoch 00004: val_loss improved from 0.00771 to 0.00760, saving model to model_checkpoints/saved_model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176/176 [==============================] - 0s 907us/sample - loss: 0.0055 - val_loss: 0.0076\n",
      "Epoch 5/5\n",
      "155/176 [=========================>....] - ETA: 0s - loss: 0.0054\n",
      "Epoch 00005: val_loss improved from 0.00760 to 0.00749, saving model to model_checkpoints/saved_model.hdf5\n",
      "176/176 [==============================] - 0s 854us/sample - loss: 0.0054 - val_loss: 0.0075\n",
      "CPU times: user 90.9 ms, sys: 2.25 ms, total: 93.2 ms\n",
      "Wall time: 93.1 ms\n",
      "Train on 177 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "125/177 [====================>.........] - ETA: 0s - loss: 0.0052\n",
      "Epoch 00001: val_loss improved from 0.00749 to 0.00721, saving model to model_checkpoints/saved_model.hdf5\n",
      "177/177 [==============================] - 1s 4ms/sample - loss: 0.0052 - val_loss: 0.0072\n",
      "Epoch 2/5\n",
      "140/177 [======================>.......] - ETA: 0s - loss: 0.0054\n",
      "Epoch 00002: val_loss improved from 0.00721 to 0.00713, saving model to model_checkpoints/saved_model.hdf5\n",
      "177/177 [==============================] - 0s 921us/sample - loss: 0.0052 - val_loss: 0.0071\n",
      "Epoch 3/5\n",
      "145/177 [=======================>......] - ETA: 0s - loss: 0.0054\n",
      "Epoch 00003: val_loss improved from 0.00713 to 0.00704, saving model to model_checkpoints/saved_model.hdf5\n",
      "177/177 [==============================] - 0s 911us/sample - loss: 0.0052 - val_loss: 0.0070\n",
      "Epoch 4/5\n",
      "145/177 [=======================>......] - ETA: 0s - loss: 0.0045\n",
      "Epoch 00004: val_loss improved from 0.00704 to 0.00696, saving model to model_checkpoints/saved_model.hdf5\n",
      "177/177 [==============================] - 0s 901us/sample - loss: 0.0045 - val_loss: 0.0070\n",
      "Epoch 5/5\n",
      "155/177 [=========================>....] - ETA: 0s - loss: 0.0046\n",
      "Epoch 00005: val_loss improved from 0.00696 to 0.00687, saving model to model_checkpoints/saved_model.hdf5\n",
      "177/177 [==============================] - 0s 843us/sample - loss: 0.0046 - val_loss: 0.0069\n",
      "CPU times: user 92.2 ms, sys: 2.18 ms, total: 94.4 ms\n",
      "Wall time: 94.1 ms\n",
      "Train on 178 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "125/178 [====================>.........] - ETA: 0s - loss: 0.0050\n",
      "Epoch 00001: val_loss improved from 0.00687 to 0.00663, saving model to model_checkpoints/saved_model.hdf5\n",
      "178/178 [==============================] - 1s 4ms/sample - loss: 0.0049 - val_loss: 0.0066\n",
      "Epoch 2/5\n",
      "145/178 [=======================>......] - ETA: 0s - loss: 0.0045\n",
      "Epoch 00002: val_loss improved from 0.00663 to 0.00654, saving model to model_checkpoints/saved_model.hdf5\n",
      "178/178 [==============================] - 0s 923us/sample - loss: 0.0044 - val_loss: 0.0065\n",
      "Epoch 3/5\n",
      "140/178 [======================>.......] - ETA: 0s - loss: 0.0038\n",
      "Epoch 00003: val_loss improved from 0.00654 to 0.00647, saving model to model_checkpoints/saved_model.hdf5\n",
      "178/178 [==============================] - 0s 923us/sample - loss: 0.0039 - val_loss: 0.0065\n",
      "Epoch 4/5\n",
      "145/178 [=======================>......] - ETA: 0s - loss: 0.0047\n",
      "Epoch 00004: val_loss improved from 0.00647 to 0.00636, saving model to model_checkpoints/saved_model.hdf5\n",
      "178/178 [==============================] - 0s 884us/sample - loss: 0.0045 - val_loss: 0.0064\n",
      "Epoch 5/5\n",
      "150/178 [========================>.....] - ETA: 0s - loss: 0.0045\n",
      "Epoch 00005: val_loss improved from 0.00636 to 0.00630, saving model to model_checkpoints/saved_model.hdf5\n",
      "178/178 [==============================] - 0s 861us/sample - loss: 0.0045 - val_loss: 0.0063\n",
      "CPU times: user 91.4 ms, sys: 2.19 ms, total: 93.6 ms\n",
      "Wall time: 93.5 ms\n",
      "Train on 179 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "135/179 [=====================>........] - ETA: 0s - loss: 0.0045\n",
      "Epoch 00001: val_loss improved from 0.00630 to 0.00626, saving model to model_checkpoints/saved_model.hdf5\n",
      "179/179 [==============================] - 1s 4ms/sample - loss: 0.0042 - val_loss: 0.0063\n",
      "Epoch 2/5\n",
      "145/179 [=======================>......] - ETA: 0s - loss: 0.0038\n",
      "Epoch 00002: val_loss improved from 0.00626 to 0.00621, saving model to model_checkpoints/saved_model.hdf5\n",
      "179/179 [==============================] - 0s 908us/sample - loss: 0.0039 - val_loss: 0.0062\n",
      "Epoch 3/5\n",
      "145/179 [=======================>......] - ETA: 0s - loss: 0.0040\n",
      "Epoch 00003: val_loss improved from 0.00621 to 0.00614, saving model to model_checkpoints/saved_model.hdf5\n",
      "179/179 [==============================] - 0s 900us/sample - loss: 0.0040 - val_loss: 0.0061\n",
      "Epoch 4/5\n",
      "140/179 [======================>.......] - ETA: 0s - loss: 0.0035\n",
      "Epoch 00004: val_loss improved from 0.00614 to 0.00608, saving model to model_checkpoints/saved_model.hdf5\n",
      "179/179 [==============================] - 0s 906us/sample - loss: 0.0035 - val_loss: 0.0061\n",
      "Epoch 5/5\n",
      "145/179 [=======================>......] - ETA: 0s - loss: 0.0041\n",
      "Epoch 00005: val_loss improved from 0.00608 to 0.00601, saving model to model_checkpoints/saved_model.hdf5\n",
      "179/179 [==============================] - 0s 873us/sample - loss: 0.0038 - val_loss: 0.0060\n",
      "CPU times: user 93.2 ms, sys: 2.18 ms, total: 95.4 ms\n",
      "Wall time: 95.4 ms\n"
     ]
    }
   ],
   "source": [
    "# day-to-day training and prediction\n",
    "\n",
    "# initialize julia instances\n",
    "j = julia.Julia(compiled_modules=False)\n",
    "\n",
    "# iterate through the remaining time steps\n",
    "for i in tqdm.tqdm(range(len(features)-burn_in_length)):\n",
    "    \n",
    "    # extract training features for the current time-step (includes all instances up to the current time-step)\n",
    "    training_features = features.iloc[0:burn_in_length+i,:]\n",
    "    # extract response variables (log returns) for the current time-step\n",
    "    training_response = log_returns.iloc[0:burn_in_length+i,:]\n",
    "    \n",
    "    # generate training epoch of sliding windows for current time-step\n",
    "    X, y = generate_epoch(training_features, training_response, n_timesteps, look_ahead_time)\n",
    "    \n",
    "    # load latest model weights\n",
    "    model = tf.keras.models.load_model(wkdir + \"model_checkpoints/saved_model.hdf5\")\n",
    "    \n",
    "    # fit to features for current time-step\n",
    "    model.fit(X, y,\n",
    "          validation_split = validation_split,\n",
    "          epochs = n_epochs,\n",
    "          batch_size = batch_size,\n",
    "          callbacks = [checkpoints])\n",
    "    \n",
    "    # extract most recent window of features for current time-step prediction\n",
    "    pred_window = np.array(features.iloc[-n_timesteps:,:]).reshape(1,n_timesteps,features.shape[1])\n",
    "    # extract log returns for the last 'vol_window' time-steps for current 'vol_window'-day volatility\n",
    "    returns_vol_window = np.array(log_returns.iloc[-vol_window:,:])\n",
    "    \n",
    "    # predict mu for tomorrow\n",
    "    mu = model.predict(pred_window)\n",
    "    # find current sigma \n",
    "    sigma = np.dot(np.transpose(returns_vol_window),returns_vol_window)\n",
    "    \n",
    "    # save optimization params to tmp\n",
    "    np.savetxt(\"tmp/mu.txt\",mu)\n",
    "    np.savetxt(\"tmp/sigma.txt\",sigma)\n",
    "    np.savetxt(\"tmp/delta.txt\",delta)\n",
    "    np.savetxt(\"tmp/min_weight.txt\",min_weight)\n",
    "    \n",
    "    # run mvo julia optimizer\n",
    "    %time j.include(\"mvo.jl\")\n",
    "    \n",
    "    # pull in optimal weights from optimizer for rebalancing\n",
    "    with open(\"tmp/weights.txt\", 'r') as f:\n",
    "        w = f.readlines()\n",
    "        weights = [float(e.replace('\\n',\"\")) for e in w]\n",
    "    \n",
    "    # append rebalanced weights to daily_portfolio_weights object\n",
    "    daily_portfolio_weights.append(weights)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move daily portfolio weights to df\n",
    "\n",
    "daily_portfolio_weights_df = pd.DataFrame(daily_portfolio_weights, columns = log_returns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "\n",
    "daily_portfolio_weights_df.to_csv(\"daily_portfolio_weights.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Mini-batch gradient descent:\n",
    "\n",
    "def fetch_batch(batch_instance, batch_size, features, log_returns, n_timesteps, look_ahead_time):\n",
    "    \n",
    "    \"\"\"\n",
    "    batch_instance: which batch we are in out of the total number of batches in the epoch (starts from 0)\n",
    "    batch_size: the number of instances in each batch\n",
    "    n_timesteps: length of series used for prediction (i.e. how many days we're predict off of)\n",
    "    look_ahead_time: number of days in advance we will predict\n",
    "    features: features matrix\n",
    "    log_returns: response var matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # begin with empty arrays to which we will append \n",
    "    X = np.array([]) \n",
    "    y = np.array([])\n",
    "    \n",
    "    for i in range((batch_instance*batch_size), (batch_instance*batch_size) + batch_size):\n",
    "        \n",
    "        ### e.g.: range[0,20), range[20,40), range[40,60) etc if batch_size is 20 ###\n",
    "        ### note that range function is NOT inclusive of last integer ###\n",
    "        \n",
    "        end_index = i + n_timesteps\n",
    "        \n",
    "        ### e.g.: end_index = 0 + 10 = 10 for first batch instance and n_timesteps = 10 ###\n",
    "        \n",
    "        # return (X_one:[n_timesteps,n_features], y_one:[look_ahead_time, n_assets]) starting on day 'i'\n",
    "        X_one, y_one = sliding_window(features, log_returns, end_index, n_timesteps, look_ahead_time)\n",
    "        \n",
    "        # append X_one:[n_timesteps,n_features] to batch ndarray X\n",
    "        X = np.append(X,X_one)\n",
    "        # append y_one:[look_ahead_time, n_assets] to batch ndarray y\n",
    "        y = np.append(y,y_one)\n",
    "    \n",
    "    # reshape to 3D with number_of_batch_instances x n_time_steps x n_features\n",
    "    X = X.reshape(batch_size, n_timesteps, features.shape[1])\n",
    "    # reshape to 2D with number_of_batch_instances x n_features\n",
    "    y = y.reshape(batch_size, log_returns.shape[1])\n",
    "    \n",
    "    return X, y \n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# start by iterating through epochs\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # then iterate through n_batches\n",
    "    for batch_instance in range(n_batches):\n",
    "        \n",
    "        # fetch the batches\n",
    "        X_batch, y_batch = fetch_batch(batch_instance, batch_size, features, \n",
    "                                       log_returns, n_timesteps, look_ahead_time)\n",
    "        \n",
    "        # fit batches to model\n",
    "        model.fit(X_batch, y_batch, batch_size=batch_size)\n",
    "        \n",
    "\"\"\"       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
